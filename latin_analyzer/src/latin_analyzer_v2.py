#!/usr/bin/env python3
"""
Analyseur de textes latins m√©di√©vaux - Version 2.1
================================================

Syst√®me int√©gr√© utilisant :
- PyCollatinus pour l'analyse morphologique
- Dictionnaire Du Cange (99k+ entr√©es de latin m√©di√©val)
- Syst√®me de scoring multi-crit√®res
- Colorisation √† 3 niveaux (rouge/orange/noir)
- Support XML Pages avec extraction MainZone automatique
- Normalisation u/v et i/j
- Fusion des mots avec tirets
- Filtrage des chiffres romains

Auteur: Claude
Date: 2025-11-25
"""

import argparse
import os
import re
import sys
import docx
from docx.shared import RGBColor, Cm, Pt
from docx.enum.text import WD_LINE_SPACING
from pathlib import Path
from collections import Counter
from typing import List, Tuple, Optional

# Ajouter PyCollatinus au path
sys.path.insert(0, '/tmp/collatinus-python')

from pycollatinus import Lemmatiseur

# Import local si ex√©cut√© comme script, sinon import relatif
try:
    from page_xml_parser import PageXMLParser
except ImportError:
    from .page_xml_parser import PageXMLParser


# Patterns regex compil√©s
PATTERNS = {
    'hyphenated_word': re.compile(r'(.*)(\w+)-\s*$'),
    'roman_numeral': re.compile(r'^[ivxlcdm]+\.$'),
}


class LatinAnalyzer:
    """Analyseur de textes latins avec d√©tection intelligente des erreurs."""

    def __init__(self, ducange_dict_file=None):
        """
        Initialise l'analyseur.

        Args:
            ducange_dict_file (str): Chemin vers le dictionnaire Du Cange
        """
        print("üîÑ Initialisation de l'analyseur latin...")

        # Charger PyCollatinus
        print("  üìö Chargement de Collatinus (latin classique)...")
        self.lemmatizer = Lemmatiseur()
        print("  ‚úÖ Collatinus charg√©")

        # Charger le dictionnaire m√©di√©val
        self.medieval_dict = set()
        if ducange_dict_file and os.path.exists(ducange_dict_file):
            print(f"  üìö Chargement du dictionnaire Du Cange...")
            with open(ducange_dict_file, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip().lower()
                    if word:
                        # Ajouter aussi les variantes normalis√©es
                        self.medieval_dict.add(word)
                        self.medieval_dict.add(self.normalize_word(word))
            print(f"  ‚úÖ {len(self.medieval_dict)} mots m√©di√©vaux charg√©s")
        else:
            print("  ‚ö†Ô∏è  Dictionnaire Du Cange non trouv√©")

        # Suffixes typiques du latin m√©di√©val
        self.medieval_suffixes = [
            'arius', 'aria', 'arium',
            'atio', 'ationis',
            'tor', 'toris',
            'torium', 'torii',
            'mentum', 'menti',
            'itia', 'itiae',
        ]

        # Contexte eccl√©siastique
        self.ecclesiastical_words = {
            'abbas', 'abbatia', 'abbatissa', 'archiepiscopus', 'basilica',
            'canonicus', 'capitulum', 'cardinalis', 'clericus', 'diaconus',
            'diocesis', 'dominus', 'ecclesia', 'episcopus', 'monasterium',
            'monachus', 'parochia', 'presbyter', 'sacerdos', 'sanctus',
        }

        print("‚úÖ Analyseur pr√™t\n")

    @staticmethod
    def normalize_word(word: str) -> str:
        """
        Normalise un mot latin en convertissant u‚Üív et i‚Üíj.

        Les variantes m√©di√©vales u/v et i/j sont consid√©r√©es comme identiques :
        - uel ‚Üí vel
        - uidetur ‚Üí videtur
        - iustus ‚Üí iustus (j‚Üíi d√©j√† fait en m√©di√©val)

        Args:
            word (str): Mot √† normaliser

        Returns:
            str: Mot normalis√©
        """
        word = word.replace('u', 'v').replace('U', 'V')
        word = word.replace('j', 'i').replace('J', 'I')
        return word

    @staticmethod
    def is_roman_numeral_with_dot(word: str) -> bool:
        """
        D√©tecte si un mot est un chiffre romain avec point final.

        G√®re les variantes m√©di√©vales avec u au lieu de v :
        - xuiii. (14)
        - uii. (7)
        - ui. (6)

        Args:
            word (str): Mot √† v√©rifier

        Returns:
            bool: True si c'est un chiffre romain avec point
        """
        # Normaliser u‚Üív pour d√©tecter les chiffres romains m√©di√©vaux
        normalized = word.lower().replace('u', 'v')
        return bool(PATTERNS['roman_numeral'].match(normalized))

    def merge_hyphenated_words(self, lines: List[str]) -> List[str]:
        """
        Fusionne les mots coup√©s avec trait d'union entre deux lignes.

        Inspir√© de xml_corpus_processor._merge_hyphenated_words()

        Args:
            lines: Liste des lignes √† traiter

        Returns:
            Liste des lignes avec mots fusionn√©s

        Example:
            >>> lines = ["sancti-", "tatis"]
            >>> analyzer.merge_hyphenated_words(lines)
            ["sanctitatis"]
        """
        processed_lines = []
        i = 0

        while i < len(lines):
            current_line = lines[i].strip() if isinstance(lines[i], str) else ''

            # D√©tection des mots coup√©s avec trait d'union
            match = PATTERNS['hyphenated_word'].search(current_line)

            if match and i + 1 < len(lines):
                prefix_text = match.group(1)  # Texte avant le mot coup√©
                prefix_word = match.group(2)  # Premi√®re partie du mot coup√©

                # R√©cup√©ration de la ligne suivante
                next_line = lines[i + 1].strip() if isinstance(lines[i + 1], str) else ''
                next_line_parts = next_line.split(' ', 1)

                if next_line_parts:
                    # Fusion du mot
                    suffix_word = next_line_parts[0]
                    fused_word = prefix_word + suffix_word

                    # Construction de la nouvelle ligne
                    new_line = prefix_text + fused_word

                    # Ajout du reste de la ligne suivante si pr√©sent
                    if len(next_line_parts) > 1:
                        new_line += ' ' + next_line_parts[1]

                    processed_lines.append(new_line)
                    i += 2  # Sauter la ligne suivante
                    continue

            # Pas de fusion n√©cessaire
            processed_lines.append(current_line)
            i += 1

        return processed_lines

    def analyze_word(self, word, context_words=None):
        """
        Analyse un mot avec scoring multi-crit√®res.

        Args:
            word (str): Le mot √† analyser
            context_words (list): Liste des mots environnants

        Returns:
            dict: {
                'word': str,
                'recognized_classical': bool,
                'recognized_medieval': bool,
                'confidence_score': int (0-100),
                'reasons': list,
                'color_code': str ('black', 'orange', 'red')
            }
        """
        clean_word = word.lower().strip()
        context_words = context_words or []

        result = {
            'word': word,
            'recognized_classical': False,
            'recognized_medieval': False,
            'confidence_score': 50,  # Score de base neutre
            'reasons': [],
            'color_code': 'orange'
        }

        # Ignorer les mots tr√®s courts
        if len(clean_word) < 2:
            result['confidence_score'] = 100
            result['color_code'] = 'black'
            result['reasons'].append("mot trop court pour √™tre analys√©")
            return result

        # Ignorer les chiffres romains avec point (xuiii., uii., ui., etc.)
        if self.is_roman_numeral_with_dot(clean_word):
            result['confidence_score'] = 100
            result['color_code'] = 'black'
            result['reasons'].append("chiffre romain")
            return result

        # Normaliser pour la comparaison (u‚Üív, i‚Üíj)
        normalized_word = self.normalize_word(clean_word)

        # Crit√®re 1 : Reconnu par Collatinus (latin classique)
        try:
            # Tester le mot original ET la version normalis√©e
            for test_word in [clean_word, normalized_word]:
                analyses = self.lemmatizer.lemmatise(test_word)
                if analyses and len(analyses) > 0:
                    result['recognized_classical'] = True
                    result['confidence_score'] += 30
                    result['reasons'].append(f"latin classique valide ({len(analyses)} analyse(s))")
                    break
        except Exception:
            pass

        # Crit√®re 2 : Pr√©sent dans le dictionnaire Du Cange (avec normalisation)
        if clean_word in self.medieval_dict or normalized_word in self.medieval_dict:
            result['recognized_medieval'] = True
            result['confidence_score'] += 40
            result['reasons'].append("pr√©sent dans le dictionnaire Du Cange")

        # Crit√®re 3 : Suffixe m√©di√©val typique
        for suffix in self.medieval_suffixes:
            if normalized_word.endswith(suffix):
                result['confidence_score'] += 10
                result['reasons'].append(f"suffixe m√©di√©val productif (-{suffix})")
                break

        # Crit√®re 4 : Contexte eccl√©siastique
        context_ecclesiastical = any(
            self.normalize_word(w.lower()) in self.ecclesiastical_words
            for w in context_words
        )
        if context_ecclesiastical:
            result['confidence_score'] += 5
            result['reasons'].append("contexte eccl√©siastique")

        # Crit√®re 5 : Variante orthographique m√©di√©vale (ae‚Üíe, ti‚Üíci, etc.)
        if self._is_medieval_variant(normalized_word):
            result['confidence_score'] += 10
            result['reasons'].append("variante orthographique m√©di√©vale d√©tect√©e")

        # D√©terminer la couleur selon le score
        score = min(result['confidence_score'], 100)
        if score >= 75:
            result['color_code'] = 'black'
        elif score >= 40:
            result['color_code'] = 'orange'
        else:
            result['color_code'] = 'red'

        result['confidence_score'] = score
        return result

    def _is_medieval_variant(self, word):
        """
        D√©tecte si un mot pourrait √™tre une variante orthographique m√©di√©vale.

        Args:
            word (str): Le mot √† analyser (d√©j√† normalis√©)

        Returns:
            bool: True si variante d√©tect√©e
        """
        # G√©n√©rer des variantes classiques et v√©rifier si elles sont reconnues
        variants = []

        # ae ‚Üê e (medieval ‚Üí mediaeval)
        if 'e' in word and 'ae' not in word:
            variants.append(word.replace('e', 'ae', 1))

        # ti ‚Üê ci (gracia ‚Üí gratia)
        if 'ci' in word:
            variants.append(word.replace('ci', 'ti'))

        # V√©rifier si une variante existe dans les dictionnaires
        for variant in variants:
            if variant in self.medieval_dict:
                return True
            try:
                analyses = self.lemmatizer.lemmatise(variant)
                if analyses and len(analyses) > 0:
                    return True
            except Exception:
                pass

        return False

    def extract_and_process_xml(self, xml_path, column_mode='single'):
        """
        Extrait le texte depuis XML Pages et traite les tirets.

        Args:
            xml_path (str): Chemin vers fichier XML ou dossier
            column_mode (str): 'single' ou 'dual'

        Returns:
            List[str]: Lignes de texte trait√©es
        """
        print(f"üìÑ Extraction du texte depuis XML Pages...")

        parser = PageXMLParser(column_mode=column_mode)

        # Extraire le texte
        if os.path.isfile(xml_path):
            lines, metadata = parser.parse_file(xml_path)
            print(f"  ‚úÖ 1 fichier trait√©, {len(lines)} lignes extraites")
        elif os.path.isdir(xml_path):
            text, metadata_list = parser.parse_folder(xml_path)
            lines = text.split('\n')
            print(f"  ‚úÖ {len(metadata_list)} fichiers trait√©s, {len(lines)} lignes extraites")
        else:
            raise ValueError(f"Chemin invalide : {xml_path}")

        # Fusionner les mots avec tirets
        print(f"üîó Fusion des mots avec tirets...")
        lines = self.merge_hyphenated_words(lines)
        print(f"  ‚úÖ Fusion termin√©e")

        return lines

    def analyze_page_xml(self, input_path, column_mode='single'):
        """
        Analyse un fichier ou dossier XML Pages.

        Args:
            input_path (str): Chemin vers fichier XML ou dossier
            column_mode (str): 'single' ou 'dual'

        Returns:
            dict: Statistiques et r√©sultats de l'analyse
        """
        # Extraire et traiter le texte XML
        lines = self.extract_and_process_xml(input_path, column_mode)

        # Analyser les lignes
        return self._analyze_lines(lines, source=input_path)

    def analyze_text_file(self, input_file):
        """
        Analyse un fichier texte brut complet.

        Args:
            input_file (str): Chemin vers le fichier texte

        Returns:
            dict: Statistiques et r√©sultats de l'analyse
        """
        print(f"üìÑ Analyse du fichier texte : {input_file}")

        with open(input_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # Fusionner les mots avec tirets
        print(f"üîó Fusion des mots avec tirets...")
        lines = self.merge_hyphenated_words(lines)

        return self._analyze_lines(lines, source=input_file)

    def _analyze_lines(self, lines, source):
        """
        Analyse une liste de lignes de texte.

        Args:
            lines (list): Liste des lignes √† analyser
            source (str): Source du texte (pour logs)

        Returns:
            dict: Statistiques et r√©sultats de l'analyse
        """
        print(f"üîç Analyse en cours...")
        print(f"   Source : {source}")

        results = []
        word_counter = Counter()
        score_distribution = {'black': 0, 'orange': 0, 'red': 0}

        total_words = 0

        for line_num, line in enumerate(lines, 1):
            # Extraire les mots de la ligne
            words = re.findall(r'\b[a-zA-Z√Ä-√ø]+\b', line)

            for i, word in enumerate(words):
                if len(word) < 2:
                    continue

                total_words += 1

                # Contexte = mots environnants (¬±3 mots)
                context_start = max(0, i - 3)
                context_end = min(len(words), i + 4)
                context = words[context_start:i] + words[i+1:context_end]

                # Analyser le mot
                analysis = self.analyze_word(word, context)

                results.append({
                    'line': line_num,
                    'word': word,
                    'analysis': analysis
                })

                word_counter[word.lower()] += 1
                score_distribution[analysis['color_code']] += 1

                # Afficher progression tous les 1000 mots
                if total_words % 1000 == 0:
                    print(f"  ‚è≥ {total_words} mots analys√©s...")

        if total_words == 0:
            print("‚ö†Ô∏è  Aucun mot trouv√© √† analyser")
            return {
                'results': [],
                'statistics': {
                    'total_words': 0,
                    'distribution': score_distribution,
                    'word_frequency': word_counter
                },
                'source_lines': lines
            }

        print(f"‚úÖ Analyse termin√©e : {total_words} mots trait√©s")
        print(f"\nüìä Distribution des scores :")
        print(f"  ‚úÖ Noir (bons mots)      : {score_distribution['black']} ({score_distribution['black']*100//total_words}%)")
        print(f"  ‚ö†Ô∏è  Orange (douteux)      : {score_distribution['orange']} ({score_distribution['orange']*100//total_words}%)")
        print(f"  ‚ùå Rouge (erreurs prob.) : {score_distribution['red']} ({score_distribution['red']*100//total_words}%)")

        return {
            'results': results,
            'statistics': {
                'total_words': total_words,
                'distribution': score_distribution,
                'word_frequency': word_counter
            },
            'source_lines': lines  # Garder les lignes sources pour le DOCX
        }

    def generate_docx(self, output_docx, analysis_results):
        """
        G√©n√®re un document Word avec colorisation √† 3 niveaux.

        Args:
            output_docx (str): Fichier DOCX de sortie
            analysis_results (dict): R√©sultats de l'analyse
        """
        print(f"\nüìù G√©n√©ration du document Word...")

        # R√©cup√©rer les lignes sources
        original_lines = analysis_results.get('source_lines', [])

        if not original_lines:
            print("‚ùå Erreur : Pas de lignes sources disponibles")
            return

        # Cr√©er un index des analyses par ligne et mot
        analysis_index = {}
        for item in analysis_results['results']:
            line_num = item['line']
            word = item['word'].lower()
            if line_num not in analysis_index:
                analysis_index[line_num] = {}
            analysis_index[line_num][word] = item['analysis']

        # Cr√©er le document
        doc = docx.Document()

        # Configuration de la page
        section = doc.sections[0]
        section.page_width = Cm(40)
        section.page_height = Cm(29.7)
        section.left_margin = Cm(2)
        section.right_margin = Cm(2)

        # Ajouter un titre
        title = doc.add_paragraph()
        title_run = title.add_run("Analyse de texte latin m√©di√©val")
        title_run.bold = True
        title_run.font.size = Pt(14)

        # L√©gende
        legend = doc.add_paragraph()
        legend.add_run("L√©gende : ").bold = True

        black_run = legend.add_run("Noir = OK (score ‚â•75) ")
        black_run.font.color.rgb = RGBColor(0, 0, 0)

        orange_run = legend.add_run("Orange = √Ä v√©rifier (score 40-74) ")
        orange_run.font.color.rgb = RGBColor(255, 140, 0)

        red_run = legend.add_run("Rouge = Erreur probable (score <40)")
        red_run.font.color.rgb = RGBColor(255, 0, 0)

        doc.add_paragraph("_" * 80)

        # Traiter chaque ligne
        for line_num, line in enumerate(original_lines, 1):
            paragraph = doc.add_paragraph()
            paragraph.paragraph_format.space_before = Pt(0)
            paragraph.paragraph_format.space_after = Pt(0)
            paragraph.paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE

            # D√©couper en mots et espaces
            tokens = re.findall(r'(\S+|\s+)', line)

            for token in tokens:
                if token.isspace():
                    paragraph.add_run(token)
                else:
                    # Extraire le mot pur (sans ponctuation)
                    clean_token = ''.join(c for c in token if c.isalpha()).lower()

                    # R√©cup√©rer l'analyse
                    color = RGBColor(0, 0, 0)  # Noir par d√©faut
                    if line_num in analysis_index and clean_token in analysis_index[line_num]:
                        analysis = analysis_index[line_num][clean_token]
                        if analysis['color_code'] == 'red':
                            color = RGBColor(255, 0, 0)
                        elif analysis['color_code'] == 'orange':
                            color = RGBColor(255, 140, 0)

                    run = paragraph.add_run(token)
                    run.font.color.rgb = color
                    run.font.size = Pt(10)

        # Sauvegarder
        doc.save(output_docx)
        print(f"‚úÖ Document cr√©√© : {output_docx}")


def main():
    """Fonction principale avec arguments CLI."""
    parser = argparse.ArgumentParser(
        description='Analyseur de textes latins m√©di√©vaux avec d√©tection intelligente des erreurs',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation :

  # Analyser un fichier texte
  python3 latin_analyzer_v2.py -i texte.txt -o resultat.docx -d data/ducange_data/dictionnaire_ducange.txt

  # Analyser des XML Pages (mode single)
  python3 latin_analyzer_v2.py -i corpus_xml/ -o resultat.docx -m xml-single

  # Analyser des XML Pages (mode dual - 2 colonnes)
  python3 latin_analyzer_v2.py -i corpus_xml/ -o resultat.docx -m xml-dual

  # Sans dictionnaire Du Cange
  python3 latin_analyzer_v2.py -i texte.txt -o resultat.docx
        """
    )

    parser.add_argument(
        '-i', '--input',
        required=True,
        help='Fichier texte ou dossier XML Pages √† analyser'
    )

    parser.add_argument(
        '-o', '--output',
        required=True,
        help='Fichier DOCX de sortie'
    )

    parser.add_argument(
        '-d', '--ducange',
        default=None,
        help='Chemin vers le dictionnaire Du Cange (optionnel)'
    )

    parser.add_argument(
        '-m', '--mode',
        choices=['txt', 'xml-single', 'xml-dual'],
        default='txt',
        help='Mode d\'analyse : txt (fichier texte), xml-single (XML 1 colonne), xml-dual (XML 2 colonnes)'
    )

    args = parser.parse_args()

    # V√©rifier que le fichier d'entr√©e existe
    if not os.path.exists(args.input):
        print(f"‚ùå Erreur : Le fichier/dossier d'entr√©e n'existe pas : {args.input}")
        sys.exit(1)

    # D√©terminer le chemin du dictionnaire par d√©faut si non fourni
    if args.ducange is None:
        script_dir = Path(__file__).parent.parent
        default_ducange = script_dir / "data" / "ducange_data" / "dictionnaire_ducange.txt"
        if default_ducange.exists():
            args.ducange = str(default_ducange)
            print(f"üí° Utilisation du dictionnaire par d√©faut : {args.ducange}\n")
        else:
            print("‚ö†Ô∏è  Pas de dictionnaire Du Cange sp√©cifi√© (analyse latin classique uniquement)\n")

    print("=" * 70)
    print("  ANALYSEUR DE TEXTES LATINS M√âDI√âVAUX - VERSION 2.1")
    print("  PyCollatinus + Du Cange + Scoring Multi-crit√®res")
    print("=" * 70)
    print()

    # Initialiser l'analyseur
    analyzer = LatinAnalyzer(ducange_dict_file=args.ducange)

    # Analyser selon le mode
    if args.mode == 'txt':
        analysis_results = analyzer.analyze_text_file(args.input)
    elif args.mode == 'xml-single':
        analysis_results = analyzer.analyze_page_xml(args.input, column_mode='single')
    elif args.mode == 'xml-dual':
        analysis_results = analyzer.analyze_page_xml(args.input, column_mode='dual')

    # G√©n√©rer le document Word
    analyzer.generate_docx(args.output, analysis_results)

    print("\n" + "=" * 70)
    print("‚úÖ TRAITEMENT TERMIN√â !")
    print(f"üìÅ Fichier g√©n√©r√© : {args.output}")
    print("=" * 70)

    return 0


if __name__ == "__main__":
    sys.exit(main())
