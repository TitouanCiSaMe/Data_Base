# Ajout d'identifiants aux articles de Lib√©ration

Ce dossier contient des scripts pour ajouter des identifiants uniques aux articles de Lib√©ration et les associer √† un fichier texte.

## üìÅ Fichiers

- **`add_article_ids.py`** - Version de base (simple, pour petits fichiers)
- **`add_article_ids_optimized.py`** - Version optimis√©e (recommand√©e)
- **`articles_metadata_liberation.csv`** - CSV source avec m√©tadonn√©es
- **`articles_metadata_liberation_with_ids.csv`** - CSV enrichi avec IDs
- **`articles_id_mapping.json`** - Mapping ID ‚Üî articles
- **`unmatched_articles.txt`** - Liste des articles non appari√©s

## üöÄ Usage

### Version optimis√©e (recommand√©e)

```bash
# Utilisation basique
python add_article_ids_optimized.py

# Avec param√®tres personnalis√©s
python add_article_ids_optimized.py \
    --csv articles_metadata_liberation.csv \
    --txt liberation_01012020_31122022(1).txt \
    --output ./resultats
```

### Version de base

```bash
python add_article_ids.py
```

## üìä Comparaison des versions

| Caract√©ristique | Version de base | Version optimis√©e |
|---|---|---|
| **Taille fichiers support√©e** | < 100 Mo | < 500 Mo |
| **Parsing du texte** | Algorithme simple | Structure explicite |
| **Matching** | Mots en commun | Fuzzy matching + dates |
| **Indexation** | Non | Oui (titres normalis√©s) |
| **Progression** | Non | Oui (tous les 50 articles) |
| **Taux d'appariement** | ~8-10% | ~13-15% |
| **Rapport d√©taill√©** | Basique | + fichier des non-appari√©s |

## üí° Recommandations par taille de fichier

### Petits fichiers (< 100 Mo)
‚úÖ Les deux versions fonctionnent bien

```bash
python add_article_ids.py
```

### Fichiers moyens (100 Mo - 500 Mo)
‚úÖ Utilisez la version optimis√©e

```bash
python add_article_ids_optimized.py
```

### Gros fichiers (500 Mo - 2 Go)
‚ö†Ô∏è La version optimis√©e fonctionne mais peut √™tre lente

**Optimisations possibles :**
- Augmenter la RAM disponible
- Traiter par batch (voir section ci-dessous)
- Utiliser `pandas` avec chunks

### Tr√®s gros fichiers (> 2 Go)
‚ùå Les versions actuelles ne sont pas adapt√©es

**Solutions :**
1. **D√©couper les fichiers** en parties plus petites
2. **Utiliser une base de donn√©es** (SQLite) pour l'indexation
3. **Traitement par chunks** avec pandas :

```python
# Exemple de traitement par chunks
import pandas as pd

chunk_size = 10000
for chunk in pd.read_csv('huge_file.csv', chunksize=chunk_size):
    # Traiter chaque chunk
    process_chunk(chunk)
```

## üîß Am√©lioration du taux d'appariement

Le taux d'appariement d√©pend de :

### ‚úÖ Facteurs positifs
- **P√©riodes identiques** - CSV et TXT couvrent les m√™mes dates
- **Titres identiques** - Pas de modification entre sources
- **Dates pr√©sentes** - Bonus de +20% si dates correspondent

### ‚ùå Facteurs n√©gatifs
- **P√©riodes diff√©rentes** - Ex: CSV 2024-2025 vs TXT 2020-2022
- **Titres reformul√©s** - Modifications √©ditoriales
- **Caract√®res sp√©ciaux** - Guillemets diff√©rents, accents, etc.

### üí° Pour am√©liorer l'appariement

Si votre taux est faible (<30%), v√©rifiez :

1. **Les p√©riodes** - Assurez-vous que CSV et TXT couvrent les m√™mes dates
   ```bash
   # V√©rifier les dates dans le CSV
   cut -d',' -f4 articles_metadata_liberation.csv | sort | uniq

   # V√©rifier les dates dans le TXT
   grep -E '^\d{4}-\d{2}-\d{2}$' liberation_*.txt | sort | uniq
   ```

2. **Ajuster le seuil** - Dans le code, ligne ~185 :
   ```python
   # Passer de 65% √† 55% si les titres varient beaucoup
   if best_score >= 0.55:  # Au lieu de 0.65
       return best_match
   ```

3. **Ajouter des m√©tadonn√©es** - Utiliser plus de champs (auteur, cat√©gorie, etc.)

## üìù Format des identifiants

Format : `LIB_YYYY_NNN`

- **LIB** - Pr√©fixe pour Lib√©ration
- **YYYY** - Ann√©e de l'article (ou XXXX si date inconnue)
- **NNN** - Num√©ro s√©quentiel (001, 002, etc.)

Exemples :
- `LIB_2024_001` - Premier article de 2024
- `LIB_2025_042` - 42√®me article de 2025
- `LIB_XXXX_015` - 15√®me article sans date

## üèóÔ∏è Architecture pour tr√®s gros fichiers

Si vous devez traiter r√©guli√®rement des fichiers > 2 Go, voici une architecture recommand√©e :

```python
import sqlite3
import pandas as pd

# 1. Charger le CSV dans SQLite
conn = sqlite3.connect('articles.db')
df = pd.read_csv('huge.csv', chunksize=10000)
for chunk in df:
    chunk.to_sql('articles', conn, if_exists='append', index=False)

# 2. Cr√©er des index
conn.execute('CREATE INDEX idx_title ON articles(Titre)')
conn.execute('CREATE INDEX idx_date ON articles(Date)')

# 3. Parser le fichier texte par chunks
def parse_text_chunks(filepath, chunk_size=1000000):
    with open(filepath, 'r') as f:
        while True:
            lines = f.readlines(chunk_size)
            if not lines:
                break
            yield parse_lines(lines)

# 4. Requ√™tes SQL pour le matching
cursor = conn.execute(
    "SELECT * FROM articles WHERE Titre LIKE ?",
    (f"%{title}%",)
)
```

## üêõ D√©pannage

### Probl√®me : "MemoryError"
**Solution :** R√©duire la taille du fichier ou utiliser chunks

### Probl√®me : Taux d'appariement tr√®s faible (<5%)
**Causes possibles :**
- P√©riodes diff√©rentes entre CSV et TXT
- Structure du fichier texte diff√©rente
- Encodage incorrect

**Solution :** V√©rifier manuellement quelques titres :
```bash
# Dans le CSV
head -5 articles_metadata_liberation.csv

# Dans le TXT
head -50 liberation_*.txt
```

### Probl√®me : Script tr√®s lent
**Solutions :**
- V√©rifier la RAM disponible : `free -h`
- R√©duire la taille des fichiers
- Utiliser la version optimis√©e

## üìö D√©pendances

Biblioth√®ques Python requises :

```bash
# Standard (inclus dans Python)
- csv
- json
- re
- pathlib
- argparse
- difflib

# Optionnelles pour tr√®s gros fichiers
pip install pandas  # Traitement par chunks
pip install fuzzywuzzy  # Matching avanc√©
pip install python-Levenshtein  # Acc√©l√©ration fuzzy
```

## üéØ Cas d'usage

### 1. Enrichir un CSV avec des IDs uniques
```bash
python add_article_ids_optimized.py --csv my_articles.csv
```

### 2. Associer CSV et fichier texte
```bash
python add_article_ids_optimized.py \
    --csv metadata.csv \
    --txt articles_full_text.txt
```

### 3. G√©n√©rer uniquement le mapping JSON
Le mapping est toujours g√©n√©r√© dans `articles_id_mapping.json`

### 4. Traiter plusieurs fichiers
```bash
for file in *.csv; do
    python add_article_ids_optimized.py --csv "$file"
done
```

## üìÑ License

Ce code est fourni tel quel pour le projet Data_Base.

## üë§ Auteur

Cr√©√© avec Claude pour le projet de recherche Data_Base.
